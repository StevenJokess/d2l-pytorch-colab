{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install d2l==0.13.2 -f https://d2l.ai/whl.html # installing d2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Concise Implementation of Linear Regression\n",
    ":label:`sec_linear_gluon`\n",
    "\n",
    "Broad and intense interest in deep learning for the past several years\n",
    "has inspired both companies, academics, and hobbyists\n",
    "to develop a variety of mature open source frameworks\n",
    "for automating the repetitive work of implementing\n",
    "gradient-based learning algorithms.\n",
    "In the previous section, we relied only on\n",
    "(i) `ndarray` for data storage and linear algebra;\n",
    "and (ii) auto differentiation for calculating derivatives.\n",
    "In practice, because data iterators, loss functions, optimizers,\n",
    "and neural network layers (and some whole architectures)\n",
    "are so common, modern libraries implement these components for us as well.\n",
    "\n",
    "In this section, we will show you how to implement\n",
    "the linear regression model from :numref:`sec_linear_scratch`\n",
    "concisely by using framework's high-level APIs.\n",
    "\n",
    "## Generating the Dataset\n",
    "\n",
    "To start, we will generate the same dataset as in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": "pytorch"
   },
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "true_w = torch.Tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)\n",
    "labels = labels.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Rather than rolling our own iterator,\n",
    "we can call upon the `data` module to read data.\n",
    "The first step will be to instantiate an `ArrayDataset`.\n",
    "This object's constructor takes one or more `ndarray`s as arguments.\n",
    "Here, we pass in `features` and `labels` as arguments.\n",
    "Next, we will use the `ArrayDataset` to instantiate a `DataLoader`,\n",
    "which also requires that we specify a `batch_size`\n",
    "and specify a Boolean value `shuffle` indicating whether or not\n",
    "we want the `DataLoader` to shuffle the data\n",
    "on each epoch (pass through the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 5,
    "tab": "pytorch"
   },
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"Construct a PyTorch data loader\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "Now we can use `data_iter` in much the same way as we called\n",
    "the `data_iter` function in the previous section.\n",
    "To verify that it is working, we can read and print\n",
    "the first minibatch of instances. Comparing to :numref:`sec_linear_scratch`, here we use `iter` to construct an Python iterator and then use `next` to obtain the first item from the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 8,
    "tab": "pytorch"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.9388, -0.4532],\n",
       "         [ 0.5993, -0.1559],\n",
       "         [ 0.1776,  0.3191],\n",
       "         [-1.3578, -0.1137],\n",
       "         [-0.6002,  0.8614],\n",
       "         [ 0.0515, -0.9627],\n",
       "         [-0.7090,  0.6196],\n",
       "         [-0.0970, -0.5717],\n",
       "         [ 0.7454,  0.0975],\n",
       "         [-0.9576, -1.0693]]),\n",
       " tensor([[3.8630],\n",
       "         [5.9271],\n",
       "         [3.4720],\n",
       "         [1.8610],\n",
       "         [0.0836],\n",
       "         [7.5892],\n",
       "         [0.6818],\n",
       "         [5.9603],\n",
       "         [5.3749],\n",
       "         [5.9212]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "When we implemented linear regression from scratch\n",
    "(in :numref:`sec_linear_scratch`),\n",
    "we defined our model parameters explicitly\n",
    "and coded up the calculations to produce output\n",
    "using basic linear algebra operations.\n",
    "You *should* know how to do this.\n",
    "But once your models get more complex,\n",
    "and once you have to do this nearly every day,\n",
    "you will be glad for the assistance.\n",
    "The situation is similar to coding up your own blog from scratch.\n",
    "Doing it once or twice is rewarding and instructive,\n",
    "but you would be a lousy web developer\n",
    "if every time you needed a blog you spent a month\n",
    "reinventing the wheel.\n",
    "\n",
    "For standard operations, we can use the framework's predefined layers,\n",
    "which allow us to focus especially\n",
    "on the layers used to construct the model\n",
    "rather than having to focus on the implementation.\n",
    "To define a linear model, we first import the `nn` module,\n",
    "which defines a large number of neural network layers\n",
    "(note that \"nn\" is an abbreviation for neural networks).\n",
    "We will first define a model variable `net`,\n",
    "which will refer to an instance of the `Sequential` class.\n",
    "The `Sequential` class defines a container\n",
    "for several layers that will be chained together.\n",
    "Given input data, a `Sequential` passes it through\n",
    "the first layer, in turn passing the output\n",
    "as the second layer's input and so forth.\n",
    "In the following example, our model consists of only one layer,\n",
    "so we do not really need `Sequential`.\n",
    "But since nearly all of our future models\n",
    "will involve multiple layers,\n",
    "we will use it anyway just to familiarize you\n",
    "with the most standard workflow.\n",
    "\n",
    "Recall the architecture of a single-layer network as shown in :numref:`fig_singleneuron`.\n",
    "The layer is said to be *fully-connected*\n",
    "because each of its inputs are connected to each of its outputs\n",
    "by means of a matrix-vector multiplication.\n",
    "\n",
    "![Linear regression is a single-layer neural network. ](http://d2l.ai/_images/singleneuron.svg)\n",
    ":label:`fig_singleneuron`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 11,
    "tab": "pytorch"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13,
    "tab": "pytorch"
   },
   "source": [
    "In Gluon, the fully-connected layer is defined in the `Linear` class. Note that we passed two arguments into `nn.Linear`. The first one specifies the input feature dimension, which is 2, and the second one is the output feature dimension, which is a single scalar and therefore 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "\n",
    "## Initializing Model Parameters\n",
    "\n",
    "Before using `net`, we need to initialize the model parameters,\n",
    "such as the weights and biases in the linear regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16,
    "tab": "pytorch"
   },
   "source": [
    "As we have specified the input and output dimensions when constructing `nn.Linear`. Now we access the parameters directly to specify there initial values. We first locate the layer by `net[0]`, which is the first layer in the network, and then use the `weight.data` and `bias.data` methods to access the parameters. Next we use the replace methods `uniform_` and `fill_` to overwrite parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 18,
    "tab": "pytorch"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.uniform_(0.0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20,
    "tab": "pytorch"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "\n",
    "## Defining the Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23,
    "tab": "pytorch"
   },
   "source": [
    "The `MSELoss` class compute the mean squared error, also known as squared L2 norm. In default it returns the averaged loss over examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 25,
    "tab": "pytorch"
   },
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "## Defining the Optimization Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28,
    "tab": "pytorch"
   },
   "source": [
    "Minibatch SGD and related variants\n",
    "are standard tools for optimizing neural networks\n",
    "and thus PyTorch supports SGD alongside a number of\n",
    "variations on this algorithm in the `optim` module.\n",
    "When we instantiate a SGD instance,\n",
    "we will specify the parameters to optimize over\n",
    "(obtainable from our net via `net.parameters()`), with a dictionary of hyper-parameters\n",
    "required by our optimization algorithm.\n",
    "SGD just requires that we set the value `learning_rate`,\n",
    "(here we set it to 0.03)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 30,
    "tab": "pytorch"
   },
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr = .03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "## Training\n",
    "\n",
    "You might have noticed that expressing our model through Gluon\n",
    "requires comparatively few lines of code.\n",
    "We did not have to individually allocate parameters,\n",
    "define our loss function, or implement stochastic gradient descent.\n",
    "Once we start working with much more complex models,\n",
    "Gluon's advantages will grow considerably.\n",
    "However, once we have all the basic pieces in place,\n",
    "the training loop itself is strikingly similar\n",
    "to what we did when implementing everything from scratch.\n",
    "\n",
    "To refresh your memory: for some number of epochs,\n",
    "we will make a complete pass over the dataset (train_data),\n",
    "iteratively grabbing one minibatch of inputs\n",
    "and the corresponding ground-truth labels.\n",
    "For each minibatch, we go through the following ritual:\n",
    "\n",
    "* Generate predictions by calling `net(X)` and calculate the loss `l` (the forward pass).\n",
    "* Calculate gradients by calling `l.backward()` (the backward pass).\n",
    "* Update the model parameters by invoking our SGD optimizer (note that `trainer` already knows which parameters to optimize over, so we just need to pass in the minibatch size.\n",
    "\n",
    "For good measure, we compute the loss after each epoch and print it to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 33,
    "tab": "pytorch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.00030276633333414793\n",
      "epoch 2, loss 0.00010191863111685961\n",
      "epoch 3, loss 0.00010210847540292889\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X) ,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print('epoch {}, loss {}'.format(epoch, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "Below, we compare the model parameters learned by training on finite data\n",
    "and the actual parameters that generated our dataset.\n",
    "To access parameters with Gluon,\n",
    "we first access the layer that we need from `net`\n",
    "and then access that layer's weight (`weight`) and bias (`bias`).\n",
    "To access each parameter's values as an `ndarray`,\n",
    "we invoke its `data` method.\n",
    "As in our from-scratch implementation,\n",
    "note that our estimated parameters are\n",
    "close to their ground truth counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 36,
    "tab": "pytorch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in estimating w tensor([[-8.7738e-05, -2.6417e-04]])\n",
      "Error in estimating b tensor([-0.0006])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "print('Error in estimating w', true_w.reshape(w.shape) - w)\n",
    "b = net[0].bias.data\n",
    "print('Error in estimating b', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 37
   },
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 39,
    "tab": "pytorch"
   },
   "source": [
    "* Using PyTorch's high-level APIs, we can implement models much more succinctly.\n",
    "* In PyTorch, the `data` module provides tools for data processing, the `nn` module defines a large number of neural network layers and common loss functions.\n",
    "* We can initialize the parameters by replacing their values with methods ending with `_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "\n",
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 42,
    "tab": "pytorch"
   },
   "source": [
    "1. If we replace `nn.MSELoss()` with `nn.MSELoss(reduction='mean')`, how can we change the learning rate for the code to behave identically. Why?\n",
    "1. Review the PyTorch documentation to see what loss functions and initialization methods are provided. Replace the loss by Huber's loss.\n",
    "1. How do you access the gradient of `net[0].weight`?\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}